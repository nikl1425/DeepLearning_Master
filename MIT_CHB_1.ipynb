{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHB MIT DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load edf data\n",
    "2. define class sz/nsz\n",
    "3. insert class cols\n",
    "4. concat\n",
    "5. windowing\n",
    "6. ensemple learn resnet152, inceptionv3, incep+resnet\n",
    "7. define OHEM loss\n",
    "8. build network\n",
    "9. evaluate\n",
    "10. calculate for confussion matrices (f1-score etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from skimage.restoration import (denoise_wavelet, estimate_sigma)\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models, Sequential\n",
    "\n",
    "\n",
    "\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\45535\\\\Desktop\\\\speciale\\\\DeepLearning_Master'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "FREQ = 256\n",
    "database_path = 'Dataset/CHB-MIT/chb-mit-scalp-eeg-database-1.0.0/'\n",
    "filtered_database_path = 'Dataset/CHB-MIT/Filtered-chb-mit/'\n",
    "filted_db_parquet_path = \"Dataset/CHB-MIT/dataframe-parquet\"\n",
    "edf_file_type = \".edf\"\n",
    "patient_one_path = 'chb04/'\n",
    "summary_txt_file_type = \"-summary.txt\"\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_patient_folder_names(database_path):\n",
    "    folders = os.listdir(database_path)\n",
    "    patient_folder_names = [(database_path + \"/\" + x) for x in folders if (x.find(\".DS_Store\") == -1)]\n",
    "    return patient_folder_names\n",
    "\n",
    "def get_all_file_names(directory):\n",
    "    files = os.listdir(directory)\n",
    "    edfFileNameList = [(directory + \"/\" + x) for x in files if (x.endswith(edf_file_type))]\n",
    "    summary_info_file_name = [(directory + \"/\" + x) for x in files if (x.endswith(summary_txt_file_type))]\n",
    "    return (summary_info_file_name[0], edfFileNameList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Den angivne sti blev ikke fundet: 'Dataset/CHB-MIT/Filtered-chb-mit/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-bddc62237e56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Testing:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msummary_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medf_file_paths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_all_file_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_all_patient_folder_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_database_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"summary file name: {summary_file_path} edf file name: {edf_file_paths[0]}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-ed92507684d2>\u001b[0m in \u001b[0;36mget_all_patient_folder_names\u001b[1;34m(database_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_all_patient_folder_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatabase_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mfolders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatabase_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mpatient_folder_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatabase_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfolders\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".DS_Store\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpatient_folder_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Den angivne sti blev ikke fundet: 'Dataset/CHB-MIT/Filtered-chb-mit/'"
     ]
    }
   ],
   "source": [
    "# Testing:\n",
    "summary_file_path, edf_file_paths = get_all_file_names(get_all_patient_folder_names(filtered_database_path)[0])\n",
    "\n",
    "print(f\"summary file name: {summary_file_path} edf file name: {edf_file_paths[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadEdfFile(FileName, print_reader_info = False):\n",
    "    try:\n",
    "        if(print_reader_info):\n",
    "            data = mne.io.read_raw_edf(FileName)\n",
    "            raw_data = data.get_data()\n",
    "            converted_raw = pd.DataFrame(raw_data.transpose(), columns=data.ch_names)\n",
    "            return converted_raw\n",
    "        else:\n",
    "            data = mne.io.read_raw_edf(FileName, verbose='error')\n",
    "            raw_data = data.get_data()\n",
    "            converted_raw = pd.DataFrame(raw_data.transpose(), columns=data.ch_names)\n",
    "            return converted_raw\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBLEM! bad channel: solution find conda mne and modify package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.4\n"
     ]
    }
   ],
   "source": [
    "print(mne.__version__)\n",
    "bla = \"Dataset/CHB-MIT/Filtered-chb-mit//chb18/chb18_01.edf\"\n",
    "ReadEdfFile(bla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'edf_file_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c60cc8432333>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Testing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReadEdfFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medf_file_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_reader_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'edf_file_paths' is not defined"
     ]
    }
   ],
   "source": [
    "# Testing \n",
    "test_df = ReadEdfFile(edf_file_paths[8], print_reader_info=False)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Usage & Loading Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edf files takes a lot of memory (look below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_df.info(memory_usage='deep'))\n",
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add col ['timems', class] - Reduce dtypes - Compress + Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.io.parquet.FastParquetImpl"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.io.parquet.get_engine('auto').__class__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce dtypes, save memory, uptimize loading times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast_dtypes(df):\n",
    "    _start = df.memory_usage(deep=True).sum() / 1024 ** 2\n",
    "    float_cols = [c for c in df if df[c].dtype == 'float64']\n",
    "    int_cols = [c for c in df if df[c].dtype in ['int64', 'int32']]\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols] = df[int_cols].astype(np.int16)\n",
    "    _end = df.memory_usage(deep=True).sum() / 1024 ** 2\n",
    "    saved_time = (_start - _end) / _start * 100\n",
    "    #print(f\"Saved: {saved_time:.2f}%\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing:\n",
    "test_df = downcast_dtypes(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert time ms in dataframe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_format_info_file(txt_summary_file_path):\n",
    "    str_container = \"\"\n",
    "    with open(txt_summary_file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            str_container += str(line).replace(\"\\n\", \"<br>\")\n",
    "\n",
    "    formatted_str = re.findall('(.*?)<br><br>', str_container)\n",
    "    bla = [x.group() for x in re.finditer('(.*?)<br><br>', str_container)]\n",
    "    for index, line in enumerate(formatted_str):\n",
    "        if re.match('(^Channels in EDF Files:|^Channels changed:)', line):\n",
    "            formatted_str.remove(formatted_str[index])\n",
    "        else:\n",
    "            pass\n",
    "    return formatted_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Dataset/CHB-MIT/Filtered-chb-mit//chb20/chb20-summary.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f62389b458e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Testing:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mread_format_info_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_database_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/chb20/chb20-summary.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-7e3da08a024b>\u001b[0m in \u001b[0;36mread_format_info_file\u001b[1;34m(txt_summary_file_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_format_info_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt_summary_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mstr_container\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt_summary_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mstr_container\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"<br>\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Dataset/CHB-MIT/Filtered-chb-mit//chb20/chb20-summary.txt'"
     ]
    }
   ],
   "source": [
    "# Testing:\n",
    "read_format_info_file(filtered_database_path + \"/chb20/chb20-summary.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary_file_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-e7ee1b864fdb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Testing:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mread_format_info_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"index: {index} line: {obj}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'summary_file_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Testing:\n",
    "for index, obj in enumerate(read_format_info_file(summary_file_path)):\n",
    "    print(f\"index: {index} line: {obj}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get sz information for [patient]-summary.txt files. </br>\n",
    "Then get information on seizures timestamp and the file name </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileInformationContainer:\n",
    "    def __init__(self, information_str):\n",
    "        self.information_str = self.clean_string(information_str)\n",
    "        self.file_name = self.set_filename()\n",
    "        self.time_start = self.set_file_time_start_ms()\n",
    "        self.sz_info = self.set_sz_information()\n",
    "        \n",
    "    def clean_string(self, uncleaned_str):\n",
    "        return uncleaned_str.replace(\"<br>\", \" \")\n",
    "\n",
    "    def set_filename(self):\n",
    "        filename_found = re.match(r\"^File Name: (.+?).edf\", self.information_str)\n",
    "        if filename_found:\n",
    "            return filename_found.group(1)\n",
    "        else:\n",
    "            print(f\"{self.file_name} failed get_filename\")\n",
    "            return \"filename not found\"\n",
    "    \n",
    "    def get_milli_sec(self, time_str):\n",
    "        \"\"\"Get Seconds from time.\"\"\"\n",
    "        h, m, s = time_str.split(':')\n",
    "        return (int(h) * 3600 + int(m) * 60 + int(s)) * 1000\n",
    "\n",
    "    def set_file_time_start_ms(self):\n",
    "        time_start_found = re.match(r\".*File Start Time: (.*?) File\", self.information_str)\n",
    "        if time_start_found:\n",
    "            try:\n",
    "                return self.get_milli_sec(time_start_found.group(1))\n",
    "            except Exception as e :\n",
    "                print(f\"{self.file_name}: error {e} cannot convert to ms time\")\n",
    "                return f\"{e}\"\n",
    "        else:\n",
    "            print(f\"{self.file_name} failed get_file_time_start_ms\")\n",
    "            return \"time start not found\"\n",
    "    \n",
    "    def get_sz_count(self):\n",
    "        sz_count = 0\n",
    "        count_found = re.search(r\".*Seizures in File: (.*?) Seizure\", self.information_str)\n",
    "        if count_found:\n",
    "            sz_count = count_found.group(1)\n",
    "        if int(sz_count) > 0:\n",
    "            return int(sz_count)\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "\n",
    "    def set_sz_information(self):\n",
    "        if(type(self.get_sz_count()) != None and self.get_sz_count() > 0):\n",
    "            try:\n",
    "                pattern = re.compile(r\"Seizure [1-9] (?P<state>[?:Start|End]+) Time: (?P<Sec>[0-9]+)\")\n",
    "                myList = [m.groupdict() for m in pattern.finditer(self.information_str)]\n",
    "                if(len(myList) <= 0):\n",
    "                     pattern = re.compile(r\"Seizure (?P<state>[?:Start|End]+) Time: (?P<Sec>[0-9]+)\")\n",
    "                     myList = [m.groupdict() for m in pattern.finditer(self.information_str)]\n",
    "                for item in myList:\n",
    "                    converted_time = int(item.get(\"Sec\")) * 1000\n",
    "                    item[\"Sec\"] = converted_time\n",
    "                formatted = []\n",
    "                for i in range(0, len(myList), 2):\n",
    "                    formatted.append({\"sz_start\" : myList[i][\"Sec\"], \"sz_end\" : myList[i+1][\"Sec\"]})\n",
    "                return formatted\n",
    "            except Exception as e:\n",
    "                print(f\"set_sz_information failed at file: {self.file_name} with the following exception: {e}\")\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def get(self):\n",
    "        return self.information_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object with sz \n",
      " info-str: File Name: chb20_01.edf File Start Time: 16:55:08 File End Time: 17:55:14 Number of Seizures in File: 0\n",
      " file-name: chb20_01\n",
      " time-start: 60908000\n",
      " sz-information: [] \n",
      "\n",
      "-------------------------------------\n",
      "Object with sz \n",
      " info-str: File Name: chb20_04.edf File Start Time: 19:55:59 File End Time: 20:55:59 Number of Seizures in File: 0\n",
      " file-name: chb20_04\n",
      " time-start: 71759000\n",
      " sz-information: []\n"
     ]
    }
   ],
   "source": [
    "# Testing:\n",
    "w_no_sz_str = FileInformationContainer(read_format_info_file(summary_file_path)[1])\n",
    "print(f\"Object with sz \\n info-str: {w_no_sz_str.information_str}\\n file-name: {w_no_sz_str.file_name}\\n time-start: {w_no_sz_str.time_start}\\n sz-information: {w_no_sz_str.sz_info} \\n\")\n",
    "print(\"-------------------------------------\")\n",
    "w_sz_str = FileInformationContainer(read_format_info_file(summary_file_path)[4])\n",
    "print(f\"Object with sz \\n info-str: {w_sz_str.information_str}\\n file-name: {w_sz_str.file_name}\\n time-start: {w_sz_str.time_start}\\n sz-information: {w_sz_str.sz_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chb20_04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "71759000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Test object:\n",
    "obj = FileInformationContainer(read_format_info_file(summary_file_path)[4])\n",
    "print(obj.file_name)\n",
    "obj.time_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert timestamp in files based of freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-6a00c9e3a602>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0minsert_time_stamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_start\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "def insert_time_stamp(dataframe, file_start_time):\n",
    "    period_row_increment_value =  (1 / 256) * 1000\n",
    "    dataframe.insert(0, \"timestamp\", [file_start_time + i * period_row_increment_value for i in dataframe.index])\n",
    "    \n",
    "    \n",
    "insert_time_stamp(test_df, obj.time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>FP1-F7</th>\n",
       "      <th>F7-T7</th>\n",
       "      <th>T7-P7</th>\n",
       "      <th>P7-O1</th>\n",
       "      <th>.-0</th>\n",
       "      <th>FP1-F3</th>\n",
       "      <th>F3-C3</th>\n",
       "      <th>C3-P3</th>\n",
       "      <th>P3-O1</th>\n",
       "      <th>...</th>\n",
       "      <th>FP2-F8</th>\n",
       "      <th>F8-T8</th>\n",
       "      <th>T8-P8</th>\n",
       "      <th>P8-O2</th>\n",
       "      <th>.-4</th>\n",
       "      <th>P7-T7</th>\n",
       "      <th>T7-FT9</th>\n",
       "      <th>FT9-FT10</th>\n",
       "      <th>FT10-T8</th>\n",
       "      <th>STI 014</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.175900e+07</td>\n",
       "      <td>5.919414e-05</td>\n",
       "      <td>6.388278e-05</td>\n",
       "      <td>-3.809524e-05</td>\n",
       "      <td>-4.825397e-05</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>8.400489e-06</td>\n",
       "      <td>5.489622e-05</td>\n",
       "      <td>-3.711844e-06</td>\n",
       "      <td>-2.285714e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.738706e-05</td>\n",
       "      <td>4.004884e-05</td>\n",
       "      <td>-2.793651e-05</td>\n",
       "      <td>-5.919414e-05</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>3.848596e-05</td>\n",
       "      <td>-3.711844e-06</td>\n",
       "      <td>-1.113553e-05</td>\n",
       "      <td>-1.894994e-05</td>\n",
       "      <td>131045.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.175900e+07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.175901e+07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>5.860806e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>5.860806e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>-1.953602e-07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.175901e+07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.758242e-06</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.367521e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>5.860806e-07</td>\n",
       "      <td>9.768009e-07</td>\n",
       "      <td>-9.768009e-07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.175902e+07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>5.860806e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>-1.758242e-06</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>-1.367521e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>5.860806e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>-1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>1.953602e-07</td>\n",
       "      <td>-9.768009e-07</td>\n",
       "      <td>-1.758242e-06</td>\n",
       "      <td>3.321123e-06</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      timestamp        FP1-F7         F7-T7         T7-P7         P7-O1  \\\n",
       "0  7.175900e+07  5.919414e-05  6.388278e-05 -3.809524e-05 -4.825397e-05   \n",
       "1  7.175900e+07  1.953602e-07  1.953602e-07  1.953602e-07  1.953602e-07   \n",
       "2  7.175901e+07  1.953602e-07  1.953602e-07  1.953602e-07  5.860806e-07   \n",
       "3  7.175901e+07  1.953602e-07  1.953602e-07  1.953602e-07  1.758242e-06   \n",
       "4  7.175902e+07  1.953602e-07  5.860806e-07  1.953602e-07 -1.758242e-06   \n",
       "\n",
       "            .-0        FP1-F3         F3-C3         C3-P3         P3-O1  ...  \\\n",
       "0  1.953602e-07  8.400489e-06  5.489622e-05 -3.711844e-06 -2.285714e-05  ...   \n",
       "1  1.953602e-07  1.953602e-07  1.953602e-07  1.953602e-07  1.953602e-07  ...   \n",
       "2  1.953602e-07  1.953602e-07  1.953602e-07  1.953602e-07  5.860806e-07  ...   \n",
       "3  1.953602e-07  1.953602e-07  1.953602e-07  1.953602e-07  1.367521e-06  ...   \n",
       "4  1.953602e-07  1.953602e-07  1.953602e-07  1.953602e-07 -1.367521e-06  ...   \n",
       "\n",
       "         FP2-F8         F8-T8         T8-P8         P8-O2           .-4  \\\n",
       "0  1.738706e-05  4.004884e-05 -2.793651e-05 -5.919414e-05  1.953602e-07   \n",
       "1  1.953602e-07  1.953602e-07  1.953602e-07  1.953602e-07  1.953602e-07   \n",
       "2  1.953602e-07  1.953602e-07  1.953602e-07  1.953602e-07  1.953602e-07   \n",
       "3  1.953602e-07  1.953602e-07  1.953602e-07  1.953602e-07  1.953602e-07   \n",
       "4  1.953602e-07  5.860806e-07  1.953602e-07 -1.953602e-07  1.953602e-07   \n",
       "\n",
       "          P7-T7        T7-FT9      FT9-FT10       FT10-T8   STI 014  \n",
       "0  3.848596e-05 -3.711844e-06 -1.113553e-05 -1.894994e-05  131045.0  \n",
       "1  1.953602e-07  1.953602e-07  1.953602e-07  1.953602e-07       0.0  \n",
       "2  1.953602e-07  1.953602e-07  1.953602e-07 -1.953602e-07       0.0  \n",
       "3  1.953602e-07  5.860806e-07  9.768009e-07 -9.768009e-07       0.0  \n",
       "4  1.953602e-07 -9.768009e-07 -1.758242e-06  3.321123e-06       0.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing:\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insert class column based on the below frow paper: </br>\n",
    "Interictal = normal state </br>\n",
    "Preictal I = 30 min. before sz </br>\n",
    "Preictal II = 10 min. before sz </br>\n",
    "Seizure = sz_start -> sz_end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_class_col(dataframe, sz_info_list):\n",
    "    \n",
    "    if \"class\" not in dataframe.columns:\n",
    "        dataframe.insert(0, \"class\", np.nan)\n",
    "\n",
    "    if len(sz_info_list) == 0:\n",
    "        dataframe.loc[(dataframe['class'] != \"seizure\") & (dataframe['class'] != \"Preictal I\") & (dataframe['class'] != \"Preictal II\"), \"class\"] = \"Interictal\"\n",
    "    else:\n",
    "        for item in sz_info_list:\n",
    "            sz_start = item[\"sz_start\"]\n",
    "            sz_end = item[\"sz_end\"]\n",
    "            #print(f\"hihi: {dataframe.iloc[0]['timestamp']}\")\n",
    "            actual_sz_start = dataframe.iloc[0]['timestamp'] + sz_start\n",
    "            preictal_one_start = actual_sz_start - (30 * 60 * 1000)\n",
    "            preictal_two_start = actual_sz_start - (10 * 60 * 1000)\n",
    "            actual_sz_end = dataframe.iloc[0]['timestamp'] + sz_end\n",
    "            #print(f\"sz: {sz_start} end: {sz_end} prei1: {preictal_one_start} prei2: {preictal_two_start} actsz: {actual_sz_start} actend: {actual_sz_end}\")\n",
    "            dataframe['timestamp'] = pd.to_numeric(dataframe['timestamp'])\n",
    "\n",
    "            #INSERTING INTERICTAL\n",
    "            dataframe.loc[(dataframe['class'] != \"seizure\") & (dataframe['class'] != \"Preictal I\") & (dataframe['class'] != \"Preictal II\") & (dataframe['timestamp'] > actual_sz_end) | (dataframe['timestamp'] < actual_sz_start), \"class\"] = \"Interictal\"\n",
    "\n",
    "            #INSERTING PREICTAL I\n",
    "            dataframe.loc[(dataframe['class'] != \"seizure\") & (dataframe['class'] != \"Preictal II\")  & (dataframe['timestamp'] >= preictal_one_start) & (dataframe['timestamp'] < preictal_two_start), \"class\"] = \"Preictal I\"\n",
    "\n",
    "            #INSERTING PREICTAL II\n",
    "            dataframe.loc[(dataframe['class'] != \"seizure\") & (dataframe['timestamp'] >= preictal_two_start) & (dataframe['timestamp'] < actual_sz_start), \"class\"] = \"Preictal II\"\n",
    "\n",
    "            #INSERTING SEIZURE CLASS\n",
    "            dataframe.loc[(dataframe['timestamp'] >= actual_sz_start) & (dataframe['timestamp'] < actual_sz_end), \"class\"] = \"seizure\"\n",
    "\n",
    "           \n",
    "           # print(f\"na: {dataframe['class'].isna().sum()}\")\n",
    "            #print(f\"datapoint: {((actual_sz_end - actual_sz_start)/3906) * 1000}\")\n",
    "            #print(dataframe[\"class\"].value_counts())\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = test_df.drop(columns=\"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing:\n",
    "# filename = obj.file_name\n",
    "# count = obj.get_sz_count()\n",
    "# start = obj.sz_info[0][\"sz_start\"]\n",
    "# end = obj.sz_info[0][\"sz_end\"]\n",
    "# print(f\"filename: {filename} sz_count: {count} sz_start: {start} sz_end: {end}\")\n",
    "# insert_class_col(test_df, obj.sz_info)\n",
    "# test_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing \"Preictal I\" because seizure is very early from file time start..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_hardisk_drive_path = os.path.dirname('/Volumes/LaCie/Database/')\n",
    "\n",
    "## Test\n",
    "# file_name = \"test.txt\"\n",
    "# test  = os.path.join(external_hardisk_drive_path, file_name)\n",
    "# file1 = open(test, \"w\")\n",
    "# file1.write(\"fileinfo\")\n",
    "# file1.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compress & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_save_compress(filename, df):\n",
    "    df.to_parquet(f\"{external_hardisk_drive_path}/MIT-CHB-Parquet/{filename}.parquet.gzip\", compression=\"gzip\")\n",
    "# Exampled\n",
    "# df_save_compress(database_path, patient_one_path, \"filename\", ch01_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load all files in paper. </br> 2. For each directory (patient) extract file information to object.</br> 3. Downcast datatypes. </br>4. Insert class and timestamp cols.</br> 5.Compress and save."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying and saving all dataframes to parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bla = get_all_patient_folder_names(filtered_database_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for patient in get_all_patient_folder_names(filtered_database_path):\n",
    "    current_patient = patient\n",
    "    info_txt_path, edf_files = get_all_file_names(current_patient)\n",
    "    # read & extract information\n",
    "    info_txt = read_format_info_file(info_txt_path)\n",
    "    for line in info_txt[1:]:\n",
    "        print(line)\n",
    "        edf_info_container = FileInformationContainer(line)\n",
    "        selected_edf_path = [x for x in edf_files if (edf_info_container.file_name in x)][0]\n",
    "        edf_df = ReadEdfFile(selected_edf_path)\n",
    "        if edf_df is not None:\n",
    "            edf_df = downcast_dtypes(edf_df)\n",
    "            insert_time_stamp(edf_df, edf_info_container.time_start)\n",
    "            insert_class_col(edf_df, edf_info_container.sz_info)\n",
    "            print(f\"filename: {edf_info_container.file_name} classes: {edf_df['class'].value_counts()}\")\n",
    "            print(\"-------------------------------------------------------\")\n",
    "            df_save_compress(edf_info_container.file_name, edf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read & load compressed dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_patient_path = 'C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all compressed dataframe file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chb01_01.parquet.gzip\n",
      "chb01_02.parquet.gzip\n",
      "chb01_03.parquet.gzip\n",
      "chb01_04.parquet.gzip\n",
      "chb01_05.parquet.gzip\n",
      "chb01_06.parquet.gzip\n",
      "chb01_07.parquet.gzip\n",
      "chb01_08.parquet.gzip\n",
      "chb01_09.parquet.gzip\n",
      "chb01_10.parquet.gzip\n",
      "chb01_11.parquet.gzip\n",
      "chb01_12.parquet.gzip\n",
      "chb01_13.parquet.gzip\n",
      "chb01_14.parquet.gzip\n",
      "chb01_15.parquet.gzip\n",
      "chb01_16.parquet.gzip\n",
      "chb01_17.parquet.gzip\n",
      "chb01_18.parquet.gzip\n",
      "chb01_19.parquet.gzip\n",
      "chb01_20.parquet.gzip\n",
      "chb01_21.parquet.gzip\n",
      "chb01_22.parquet.gzip\n",
      "chb01_23.parquet.gzip\n",
      "chb01_24.parquet.gzip\n",
      "chb01_25.parquet.gzip\n",
      "chb01_26.parquet.gzip\n",
      "chb01_27.parquet.gzip\n",
      "chb01_29.parquet.gzip\n",
      "chb01_30.parquet.gzip\n",
      "chb01_31.parquet.gzip\n",
      "chb01_32.parquet.gzip\n",
      "chb01_33.parquet.gzip\n",
      "chb01_34.parquet.gzip\n",
      "chb01_36.parquet.gzip\n",
      "chb01_37.parquet.gzip\n",
      "chb01_38.parquet.gzip\n",
      "chb01_39.parquet.gzip\n",
      "chb01_40.parquet.gzip\n",
      "chb01_41.parquet.gzip\n",
      "chb01_42.parquet.gzip\n",
      "chb01_43.parquet.gzip\n",
      "chb10_01.parquet.gzip\n",
      "chb10_02.parquet.gzip\n",
      "chb10_03.parquet.gzip\n",
      "chb10_04.parquet.gzip\n",
      "chb10_05.parquet.gzip\n",
      "chb10_06.parquet.gzip\n",
      "chb10_07.parquet.gzip\n",
      "chb10_08.parquet.gzip\n",
      "chb10_12.parquet.gzip\n",
      "chb10_13.parquet.gzip\n",
      "chb10_14.parquet.gzip\n",
      "chb10_15.parquet.gzip\n",
      "chb10_16.parquet.gzip\n",
      "chb10_17.parquet.gzip\n",
      "chb10_18.parquet.gzip\n",
      "chb10_19.parquet.gzip\n",
      "chb10_20.parquet.gzip\n",
      "chb10_21.parquet.gzip\n",
      "chb10_22.parquet.gzip\n",
      "chb10_27.parquet.gzip\n",
      "chb10_28.parquet.gzip\n",
      "chb10_30.parquet.gzip\n",
      "chb10_31.parquet.gzip\n",
      "chb10_38.parquet.gzip\n"
     ]
    }
   ],
   "source": [
    "for f in os.listdir(single_patient_path):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_file_type = \".parquet.gzip\"\n",
    "files = [single_patient_path + \"/\" + f for f in os.listdir(single_patient_path) if f.endswith(compressed_file_type)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for reducing number of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cols(dataframe, col_start = 0, col_end = 0):\n",
    "    if col_end == 0:\n",
    "        col_end = len(dataframe.columns) - 1\n",
    "    \n",
    "    dataframe = dataframe.iloc[: , col_start: col_end]\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read compressed dataframes and format. Extract classes to seperate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = ['.-0','.-1', '.-2', '.-3', '.-4', 'STI 014']\n",
    "\n",
    "def read_compressed_df(path):\n",
    "    df = pd.read_parquet(path)\n",
    "    \n",
    "    # Remove STI 14 col:\n",
    "    if any(x in df.columns for x in matches):\n",
    "        for col_name in matches:\n",
    "            try:\n",
    "                df.drop(columns=col_name, inplace=True)\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    df = df[['timestamp', 'class', 'F3-C3']]\n",
    "\n",
    "    sz_df = df.loc[df['class'] == \"seizure\"].reset_index(drop=True)\n",
    "    sz_df = remove_cols(sz_df, col_end=10)\n",
    "\n",
    "    prei_one_df = df.loc[df['class'] == \"Preictal I\"]\n",
    "    prei_one_df = remove_cols(prei_one_df, col_end=10)\n",
    "\n",
    "    prei_two_df = df.loc[df['class'] == \"Preictal II\"]\n",
    "    prei_two_df = remove_cols(prei_two_df, col_end=10)\n",
    "\n",
    "    inter_df = df.loc[df['class'] == \"Interictal\"]\n",
    "    inter_df = remove_cols(inter_df, col_end=10)\n",
    "\n",
    "    channels = [item for item in list(sz_df.columns) if item != \"class\" if item != \"timestamp\"]\n",
    "\n",
    "    return (sz_df, prei_one_df, prei_two_df, inter_df, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in files[7:15]:\n",
    "    sz, prei_one, prei_two, inter, selected_channels = read_compressed_df(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = (os.path.join(root, filename)\n",
    "#     for root, _, filenames in os.walk(\"/volumes/LaCie/Database/\")\n",
    "#     for filename in filenames)\n",
    "\n",
    "# for path in paths:\n",
    "#     newname = path.replace('Database', \"\", 2)\n",
    "#     if newname != path:\n",
    "#         os.rename(path, newname)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window(channel, start_index, data, size = 4, overlap = 0, is_sezure = False, frequency = 256):\n",
    "    if(is_sezure):\n",
    "        overlap = 2\n",
    "    else:\n",
    "        overlap = overlap\n",
    "    \n",
    "    \n",
    "    start = start_index * (size - overlap) * frequency\n",
    "    end = start + (size * frequency)\n",
    "    #print(\"is_sezure: \" + str(is_sezure) + \" time: \" + str(datetime.fromtimestamp((data['timestamp'][start:start+1]/1000).tolist()[0]).strftime('%H:%M:%S')) + str(' lenthWindow: '+ str(len(data[channel][start:end].tolist()))) + ' start: ' + str(start) + \" end: \" + str(end) )\n",
    "\n",
    "    return [data[channel][start:end].tolist(), datetime.fromtimestamp((data['timestamp'][start:start+1]/1000).tolist()[0]).strftime('%H:%M:%S')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_window_iteration(dataframe, buffer):\n",
    "    len_of_df = int(len(dataframe) / (buffer*256))\n",
    "    return len_of_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_speciale_windows = \"C:/Users/45535/Desktop/speciale/DeepLearning_Master/windows\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_transform_save_to_folder(index, win, channel, patient_state, patient, plot_title = False):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    series = win[0]\n",
    "    time_of_observation = str(datetime.strptime(win[1], \"%H:%M:%S\").strftime(\"%H:%M:%S\")).replace(\":\", \"-\")\n",
    "    try:\n",
    "        series = np.array(series).astype(float)\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")\n",
    "        print(f\"patient_state: {patient_state} channel: {channel} index: {index} window: {series}\")\n",
    "    denoised_series = denoise_wavelet(series, method='BayesShrink',wavelet='db6', mode='soft',rescale_sigma=True, multichannel=False, wavelet_levels=3)\n",
    "    if plot_title:\n",
    "        plt.title(f\"{channel} : is_seizure = {patient_state} : {time_of_observation}\")\n",
    "\n",
    "    plt.specgram(denoised_series,Fs=256,cmap='jet')\n",
    "\n",
    "    if patient_state == \"seizure\":\n",
    "        plt.savefig(f'./windows/Seizure/{patient}_{index}_{channel}_{time_of_observation}.png')\n",
    "    elif patient_state == \"interictal\":\n",
    "        plt.savefig(f'./windows/Interictal/{patient}_{index}_{channel}_{time_of_observation}.png')\n",
    "    elif patient_state == \"prei_one\":\n",
    "        plt.savefig(f'./windows/Preictal_One/{patient}_{index}_{channel}_{str(time_of_observation).strip()}.png')\n",
    "    elif patient_state == \"prei_two\":\n",
    "        plt.savefig(f'./windows/Preictal_Two/{patient}_{index}_{channel}_{str(time_of_observation).strip()}.png')\n",
    "    \n",
    "    del series\n",
    "    plt.clf()    \n",
    "    plt.close()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_01.parquet.gzip index: 0\n",
      "len of sz: 0\n",
      "memory usage = 61.3 : available memory = 38.65894105451912\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_01.parquet.gzip = done : count = 0 : files left = 65 : time of creation = 2021-10-21 15:31:49.678133\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_02.parquet.gzip index: 1\n",
      "len of sz: 0\n",
      "memory usage = 67.0 : available memory = 32.96264123174214\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_02.parquet.gzip = done : count = 1 : files left = 64 : time of creation = 2021-10-21 15:36:06.466304\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_03.parquet.gzip index: 2\n",
      "len of sz: 10240\n",
      "memory usage = 67.1 : available memory = 32.87199695735461\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_03.parquet.gzip = done : count = 2 : files left = 63 : time of creation = 2021-10-21 15:40:37.553550\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_04.parquet.gzip index: 3\n",
      "len of sz: 6912\n",
      "memory usage = 72.8 : available memory = 27.159122319011555\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_04.parquet.gzip = done : count = 3 : files left = 62 : time of creation = 2021-10-21 15:45:50.866775\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_05.parquet.gzip index: 4\n",
      "len of sz: 0\n",
      "memory usage = 79.5 : available memory = 20.46039497135973\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_05.parquet.gzip = done : count = 4 : files left = 61 : time of creation = 2021-10-21 15:51:37.766724\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_06.parquet.gzip index: 5\n",
      "len of sz: 0\n",
      "memory usage = 80.3 : available memory = 19.652198304076418\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_06.parquet.gzip = done : count = 5 : files left = 60 : time of creation = 2021-10-21 15:57:52.973493\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_07.parquet.gzip index: 6\n",
      "len of sz: 0\n",
      "memory usage = 86.0 : available memory = 14.02917408102992\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_07.parquet.gzip = done : count = 6 : files left = 59 : time of creation = 2021-10-21 16:04:36.531011\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_08.parquet.gzip index: 7\n",
      "len of sz: 0\n",
      "memory usage = 88.2 : available memory = 11.828308004818004\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_08.parquet.gzip = done : count = 7 : files left = 58 : time of creation = 2021-10-21 16:11:47.596617\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_09.parquet.gzip index: 8\n",
      "len of sz: 0\n",
      "memory usage = 90.8 : available memory = 9.200297624990107\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_09.parquet.gzip = done : count = 8 : files left = 57 : time of creation = 2021-10-21 16:19:27.661946\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_10.parquet.gzip index: 9\n",
      "len of sz: 0\n",
      "memory usage = 93.2 : available memory = 6.825600464191061\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_10.parquet.gzip = done : count = 9 : files left = 56 : time of creation = 2021-10-21 16:27:35.487003\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_11.parquet.gzip index: 10\n",
      "len of sz: 0\n",
      "memory usage = 97.2 : available memory = 2.760464566340038\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_11.parquet.gzip = done : count = 10 : files left = 55 : time of creation = 2021-10-21 16:36:07.079886\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_12.parquet.gzip index: 11\n",
      "len of sz: 0\n",
      "memory usage = 97.4 : available memory = 2.5912282419461192\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_12.parquet.gzip = done : count = 11 : files left = 54 : time of creation = 2021-10-21 16:46:33.845982\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_13.parquet.gzip index: 12\n",
      "len of sz: 0\n",
      "memory usage = 96.1 : available memory = 3.882524250591004\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_13.parquet.gzip = done : count = 12 : files left = 53 : time of creation = 2021-10-21 16:56:46.558711\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_14.parquet.gzip index: 13\n",
      "len of sz: 0\n",
      "memory usage = 90.1 : available memory = 9.858575205759857\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_14.parquet.gzip = done : count = 13 : files left = 52 : time of creation = 2021-10-21 17:06:34.633568\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_15.parquet.gzip index: 14\n",
      "len of sz: 10240\n",
      "memory usage = 93.0 : available memory = 6.973089860759521\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_15.parquet.gzip = done : count = 14 : files left = 51 : time of creation = 2021-10-21 17:16:28.989045\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_16.parquet.gzip index: 15\n",
      "len of sz: 13056\n",
      "memory usage = 95.2 : available memory = 4.842179609897642\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_16.parquet.gzip = done : count = 15 : files left = 50 : time of creation = 2021-10-21 17:26:52.645438\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_17.parquet.gzip index: 16\n",
      "len of sz: 0\n",
      "memory usage = 96.1 : available memory = 3.91033337511117\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_17.parquet.gzip = done : count = 16 : files left = 49 : time of creation = 2021-10-21 17:37:36.854606\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_18.parquet.gzip index: 17\n",
      "len of sz: 23040\n",
      "memory usage = 89.3 : available memory = 10.697106912852796\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_18.parquet.gzip = done : count = 17 : files left = 48 : time of creation = 2021-10-21 17:49:04.242814\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_19.parquet.gzip index: 18\n",
      "len of sz: 0\n",
      "memory usage = 93.7 : available memory = 6.304179379437943\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_19.parquet.gzip = done : count = 18 : files left = 47 : time of creation = 2021-10-21 18:00:42.685994\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_20.parquet.gzip index: 19\n",
      "len of sz: 0\n",
      "memory usage = 95.7 : available memory = 4.259270530098205\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_20.parquet.gzip = done : count = 19 : files left = 46 : time of creation = 2021-10-21 18:09:34.978212\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_21.parquet.gzip index: 20\n",
      "len of sz: 23808\n",
      "memory usage = 96.1 : available memory = 3.8645541675316926\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_21.parquet.gzip = done : count = 20 : files left = 45 : time of creation = 2021-10-21 18:22:17.467851\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_22.parquet.gzip index: 21\n",
      "len of sz: 0\n",
      "memory usage = 90.8 : available memory = 9.24730370571018\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_22.parquet.gzip = done : count = 21 : files left = 44 : time of creation = 2021-10-21 18:35:11.521495\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_23.parquet.gzip index: 22\n",
      "len of sz: 0\n",
      "memory usage = 94.8 : available memory = 5.185848427280944\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_23.parquet.gzip = done : count = 22 : files left = 43 : time of creation = 2021-10-21 18:48:31.049126\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_24.parquet.gzip index: 23\n",
      "len of sz: 0\n",
      "memory usage = 96.2 : available memory = 3.775257047965207\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_24.parquet.gzip = done : count = 23 : files left = 42 : time of creation = 2021-10-21 19:02:18.604568\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_25.parquet.gzip index: 24\n",
      "len of sz: 0\n",
      "memory usage = 83.0 : available memory = 16.950936380854138\n",
      "filename: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_25.parquet.gzip = done : count = 24 : files left = 41 : time of creation = 2021-10-21 19:16:22.181247\n",
      "started file: C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/chb01_26.parquet.gzip index: 25\n",
      "len of sz: 25856\n",
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "try:\n",
    "    for filename in files:\n",
    "        print(\"started file: \" + str(filename) + \" index: \" + str(count))\n",
    "        sz, prei_one, prei_two, inter, selected_channels = read_compressed_df(filename)\n",
    "        print(f\"len of sz: {len(sz)}\")\n",
    "        patient = re.search('C:/Users/45535/Desktop/speciale/DeepLearning_Master/SingleChannelParquet/(.*).parquet.gzip', filename).group(1)\n",
    "        for channel in selected_channels:\n",
    "            if len(inter) > 0 and inter.empty == False:\n",
    "                inter_win = [get_window(channel=channel,start_index=i, data=inter) for i in range(get_max_window_iteration(inter, 4))]\n",
    "                for index, window in enumerate(inter_win):\n",
    "                    spec_transform_save_to_folder(win=window, index=index, channel=channel, patient_state = \"interictal\", patient=patient)\n",
    "                del inter_win\n",
    "\n",
    "            if len(sz) > 0 and sz.empty == False:\n",
    "                sz_win = [get_window(channel=channel, start_index=i, data=sz, is_sezure=True) for i in range(get_max_window_iteration(sz, 2))]\n",
    "                for index, window in enumerate(sz_win):\n",
    "                    spec_transform_save_to_folder(channel=channel, index=index, win=window, patient_state=\"seizure\", patient=patient)\n",
    "                del sz_win\n",
    "\n",
    "            if len(prei_one) > 0 and prei_one.empty == False:\n",
    "                prei_one_win = [get_window(channel=channel,start_index=i, data=inter) for i in range(get_max_window_iteration(prei_one, 4))]\n",
    "                for index, window in enumerate(prei_one_win):\n",
    "                    spec_transform_save_to_folder(channel=channel, index=index, win=window, patient_state=\"prei_one\", patient=patient)\n",
    "                del prei_one_win\n",
    "\n",
    "            if len(prei_two) > 0 and prei_two.empty == False:\n",
    "                prei_two_win = [get_window(channel=channel, start_index=i, data=inter) for i in range(get_max_window_iteration(prei_two, 4))]\n",
    "                for index, window in enumerate(prei_two_win):\n",
    "                    spec_transform_save_to_folder(channel=channel, index=index, win=window, patient_state=\"prei_two\", patient=patient)\n",
    "                del prei_two_win\n",
    "                \n",
    "        print(f\"memory usage = {psutil.virtual_memory().percent} : available memory = {psutil.virtual_memory().available * 100 / psutil.virtual_memory().total}\")\n",
    "        print(f\"filename: {filename} = done : count = {count} : files left = {len(files) - count} : time of creation = {datetime.now()}\")\n",
    "        del sz, prei_one, prei_two, inter\n",
    "        count += 1\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b6638ef1d9a6180fab9b51e4125d3ec555d02345491c5a81d972c6852d44d818"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tf28': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
